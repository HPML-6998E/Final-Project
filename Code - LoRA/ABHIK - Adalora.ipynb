{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate evaluate datasets peft -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx6n-0Ud4CoM",
        "outputId": "4c7c667b-4e33-487d-d7a1-48de18207818"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I57bDl9o39tX",
        "outputId": "ac7c4896-6b2b-4411-e36a-91fe73f99f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 663,984 || all params: 86,540,345 || trainable%: 0.7673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:  61%|██████    | 2880/4735 [04:39<02:58, 10.40it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 1: 100%|██████████| 4735/4735 [07:39<00:00, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 4.2067440153804725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  14%|█▍        | 228/1579 [00:11<01:04, 20.94it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 543/1579 [00:26<00:49, 21.00it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 50.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2:  89%|████████▉ | 4212/4735 [06:48<00:50, 10.38it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 2: 100%|██████████| 4735/4735 [07:38<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss = 3.7980484324498565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▍        | 229/1579 [00:11<01:04, 20.87it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 541/1579 [00:26<00:49, 20.96it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 62.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3:  22%|██▏       | 1031/4735 [01:40<06:01, 10.26it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 3: 100%|██████████| 4735/4735 [07:38<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss = 3.6365246208819317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▍        | 229/1579 [00:11<01:04, 20.99it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 541/1579 [00:26<00:49, 20.97it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 67.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4:   5%|▌         | 244/4735 [00:23<07:12, 10.38it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 4: 100%|██████████| 4735/4735 [07:38<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss = 3.543002602970965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  14%|█▍        | 228/1579 [00:11<01:04, 21.01it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 543/1579 [00:26<00:49, 20.74it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 70.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5:  38%|███▊      | 1815/4735 [02:55<04:42, 10.34it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 5: 100%|██████████| 4735/4735 [07:38<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss = 3.482455226511487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▍        | 229/1579 [00:11<01:04, 21.01it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 541/1579 [00:26<00:49, 21.06it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 71.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6:  70%|██████▉   | 3292/4735 [05:18<02:19, 10.37it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 6: 100%|██████████| 4735/4735 [07:38<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss = 3.442574838600038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▍        | 229/1579 [00:11<01:04, 21.08it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 541/1579 [00:26<00:49, 21.11it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 72.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7:  31%|███       | 1474/4735 [02:22<05:15, 10.34it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 7: 100%|██████████| 4735/4735 [07:38<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss = 3.4176211204045424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  15%|█▍        | 229/1579 [00:11<01:04, 20.94it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 541/1579 [00:26<00:49, 20.90it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 73.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8:  43%|████▎     | 2018/4735 [03:15<04:23, 10.32it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Training Epoch 8: 100%|██████████| 4735/4735 [07:39<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Loss = 3.4043755804221005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:  14%|█▍        | 228/1579 [00:11<01:04, 21.05it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating:  34%|███▍      | 543/1579 [00:26<00:49, 20.73it/s]/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n",
            "Evaluating: 100%|██████████| 1579/1579 [01:15<00:00, 20.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 73.60%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, get_scheduler\n",
        "from peft import AdaLoraConfig, get_peft_model\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model and dataset configuration\n",
        "model_name_or_path = \"google/vit-base-patch16-224-in21k\"\n",
        "dataset_name = \"food101\"\n",
        "batch_size = 16\n",
        "num_epochs = 8\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(dataset_name)\n",
        "processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
        "normalize = transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    examples[\"image\"] = [transform(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    return examples\n",
        "\n",
        "train_dataset = dataset[\"train\"].with_transform(preprocess_function)\n",
        "eval_dataset = dataset[\"validation\"].with_transform(preprocess_function)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "total_training_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "# Configure AdaLoRA\n",
        "peft_config = AdaLoraConfig(\n",
        "    init_r=12,\n",
        "    target_r=8,\n",
        "    beta1=0.85,\n",
        "    beta2=0.85,\n",
        "    tinit=200,\n",
        "    tfinal=1000,\n",
        "    deltaT=10,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    inference_mode=False,\n",
        "    target_modules = ['query', 'key', 'value']\n",
        ")\n",
        "\n",
        "peft_config.total_step = total_training_steps\n",
        "\n",
        "# Load the model and apply AdaLoRA\n",
        "model = AutoModelForImageClassification.from_pretrained(model_name_or_path, num_labels=101)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(train_dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "# Training loop with AdaLoRA update\n",
        "global_step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        inputs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(pixel_values=inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Update AdaLoRA matrices and allocate budget\n",
        "        model.base_model.update_and_allocate(global_step)\n",
        "        global_step += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            inputs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "            outputs = model(pixel_values=inputs)\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(f\"{model_name_or_path}_adalora_fine_tuned\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUhoehb6NvRh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}