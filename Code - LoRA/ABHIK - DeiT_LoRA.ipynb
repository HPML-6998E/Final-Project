{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ1afexfJz2s",
        "outputId": "75a56db7-e2a5-4a0a-e7bb-9ebe4ee55d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate evaluate datasets peft -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWh3-3mBQBzG",
        "outputId": "b1ed3889-1ab1-412f-a6db-518197a923f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Installing collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "import peft\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuXlMABdKFF1",
        "outputId": "19152186-e34f-45b5-aa76-17cca428c97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.46.2\n",
            "Accelerate version: 1.1.1\n",
            "PEFT version: 0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"facebook/deit-base-patch16-224\""
      ],
      "metadata": {
        "id": "9ZMF9yIaKKl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"food101\", split=\"train[:5000]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxBz97SKaXM",
        "outputId": "bf867681-3655-43b2-f0f9-8733db9da53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset.features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zK7MHSqEKqPj",
        "outputId": "9800a1df-c168-4e6a-8c72-13e1a5b242e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'baklava'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, ViTForImageClassification"
      ],
      "metadata": {
        "id": "TPkPo5c6KsuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = AutoFeatureExtractor.from_pretrained('facebook/deit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('facebook/deit-base-patch16-224')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avddWb4RMZ3H",
        "outputId": "ac315fb5-677e-4446-e680-1ab5937b7e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        Resize(image_processor.size[\"height\"]),\n",
        "        CenterCrop(image_processor.size[\"height\"]),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "    return example_batch\n",
        "\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "    return example_batch"
      ],
      "metadata": {
        "id": "J18WZfxLNAgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = dataset.train_test_split(test_size=0.1)\n",
        "train_ds = splits[\"train\"]\n",
        "val_ds = splits[\"test\"]"
      ],
      "metadata": {
        "id": "7tKsZ3T5NMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)"
      ],
      "metadata": {
        "id": "8ZXMURZpNTTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "T9HyOEymNWGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnoKCZBWNYna",
        "outputId": "ed4186aa-1aef-4e4f-dc32-a6cb3bb92ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 86567656 || all params: 86567656 || trainable%: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name, \":\", module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nB9bpu-NbOI",
        "outputId": "fe56a092-03e8-48e7-a381-8567cb817b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " : ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTSdpaAttention(\n",
            "            (attention): ViTSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n",
            "vit : ViTModel(\n",
            "  (embeddings): ViTEmbeddings(\n",
            "    (patch_embeddings): ViTPatchEmbeddings(\n",
            "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    )\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (encoder): ViTEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x ViTLayer(\n",
            "        (attention): ViTSdpaAttention(\n",
            "          (attention): ViTSdpaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (output): ViTSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): ViTIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): ViTOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.embeddings : ViTEmbeddings(\n",
            "  (patch_embeddings): ViTPatchEmbeddings(\n",
            "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.embeddings.patch_embeddings : ViTPatchEmbeddings(\n",
            "  (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            ")\n",
            "vit.embeddings.patch_embeddings.projection : Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "vit.embeddings.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder : ViTEncoder(\n",
            "  (layer): ModuleList(\n",
            "    (0-11): 12 x ViTLayer(\n",
            "      (attention): ViTSdpaAttention(\n",
            "        (attention): ViTSdpaSelfAttention(\n",
            "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (output): ViTSelfOutput(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (intermediate): ViTIntermediate(\n",
            "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (intermediate_act_fn): GELUActivation()\n",
            "      )\n",
            "      (output): ViTOutput(\n",
            "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer : ModuleList(\n",
            "  (0-11): 12 x ViTLayer(\n",
            "    (attention): ViTSdpaAttention(\n",
            "      (attention): ViTSdpaSelfAttention(\n",
            "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (output): ViTSelfOutput(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (intermediate): ViTIntermediate(\n",
            "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "      (intermediate_act_fn): GELUActivation()\n",
            "    )\n",
            "    (output): ViTOutput(\n",
            "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.0 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.0.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.0.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.0.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.0.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.0.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.0.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.1 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.1.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.1.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.1.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.1.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.1.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.1.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.2 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.2.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.2.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.2.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.2.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.2.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.2.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.3 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.3.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.3.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.3.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.3.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.3.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.3.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.4 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.4.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.4.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.4.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.4.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.4.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.4.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.5 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.5.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.5.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.5.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.5.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.5.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.5.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.6 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.6.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.6.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.6.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.6.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.6.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.6.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.7 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.7.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.7.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.7.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.7.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.7.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.7.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.8 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.8.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.8.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.8.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.8.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.8.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.8.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.9 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.9.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.9.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.9.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.9.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.9.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.9.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.10 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.10.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.10.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.10.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.10.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.10.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.10.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.11 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.11.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.11.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.11.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.11.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.11.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.11.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.layernorm : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "classifier : Linear(in_features=768, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# def extract_target_layers(model):\n",
        "#     \"\"\"\n",
        "#     Extracts layers matching attention blocks like:\n",
        "#     'encoder.layer.X.attention.attention.query',\n",
        "#     'encoder.layer.X.attention.attention.key',\n",
        "#     'encoder.layer.X.attention.attention.value'\n",
        "#     \"\"\"\n",
        "#     # Adjust the pattern based on actual printed names\n",
        "#     pattern = re.compile(r'vit\\.encoder\\.layer\\.\\d+\\.attention\\.attention\\.(query|key|value)')\n",
        "\n",
        "#     # Extract matching layers\n",
        "#     target_layers = [name for name, _ in model.named_parameters() if pattern.search(name)]\n",
        "\n",
        "#     return target_layers\n",
        "\n",
        "# # Extract the target layers\n",
        "# target_layers = extract_target_layers(model)\n",
        "# print(target_layers)\n",
        "\n",
        "def extract_target_layers(model):\n",
        "    \"\"\"\n",
        "    Extracts layer names matching the pattern:\n",
        "    'vit.encoder.layer.X.attention.attention.query'\n",
        "    'vit.encoder.layer.X.attention.attention.key'\n",
        "    'vit.encoder.layer.X.attention.attention.value'\n",
        "    without the '.weight' or '.bias' suffixes.\n",
        "    \"\"\"\n",
        "    # Define the regex pattern to match only the base layer names\n",
        "    pattern = re.compile(r'(vit\\.encoder\\.layer\\.\\d+\\.attention\\.attention\\.(query|key|value))')\n",
        "\n",
        "    # Use a set to avoid duplicates\n",
        "    target_layers = set()\n",
        "\n",
        "    # Extract matching layers from model parameters\n",
        "    for name, _ in model.named_parameters():\n",
        "        match = pattern.match(name)\n",
        "        if match:\n",
        "            # Add only the base layer name (without '.weight' or '.bias')\n",
        "            target_layers.add(match.group(1))\n",
        "\n",
        "    # Convert set to list and return\n",
        "    return list(target_layers)\n",
        "\n",
        "# Extract the target layers\n",
        "target_layers = extract_target_layers(model)\n",
        "\n",
        "# Print the extracted layers\n",
        "print(target_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1jeZIhBOU2g",
        "outputId": "36e44b0c-6b7d-4a6f-d3d7-bff1e4cf143f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vit.encoder.layer.0.attention.attention.query', 'vit.encoder.layer.8.attention.attention.key', 'vit.encoder.layer.5.attention.attention.query', 'vit.encoder.layer.2.attention.attention.key', 'vit.encoder.layer.11.attention.attention.value', 'vit.encoder.layer.2.attention.attention.value', 'vit.encoder.layer.10.attention.attention.value', 'vit.encoder.layer.8.attention.attention.query', 'vit.encoder.layer.10.attention.attention.query', 'vit.encoder.layer.5.attention.attention.value', 'vit.encoder.layer.8.attention.attention.value', 'vit.encoder.layer.4.attention.attention.value', 'vit.encoder.layer.6.attention.attention.value', 'vit.encoder.layer.9.attention.attention.key', 'vit.encoder.layer.4.attention.attention.query', 'vit.encoder.layer.3.attention.attention.query', 'vit.encoder.layer.9.attention.attention.value', 'vit.encoder.layer.11.attention.attention.query', 'vit.encoder.layer.3.attention.attention.value', 'vit.encoder.layer.9.attention.attention.query', 'vit.encoder.layer.4.attention.attention.key', 'vit.encoder.layer.6.attention.attention.query', 'vit.encoder.layer.3.attention.attention.key', 'vit.encoder.layer.7.attention.attention.query', 'vit.encoder.layer.11.attention.attention.key', 'vit.encoder.layer.2.attention.attention.query', 'vit.encoder.layer.6.attention.attention.key', 'vit.encoder.layer.1.attention.attention.key', 'vit.encoder.layer.0.attention.attention.key', 'vit.encoder.layer.7.attention.attention.value', 'vit.encoder.layer.5.attention.attention.key', 'vit.encoder.layer.1.attention.attention.query', 'vit.encoder.layer.1.attention.attention.value', 'vit.encoder.layer.0.attention.attention.value', 'vit.encoder.layer.10.attention.attention.key', 'vit.encoder.layer.7.attention.attention.key']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    target_modules = target_layers,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        ")\n",
        "lora_model = get_peft_model(model, config)\n",
        "print_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGLDEtEeOoHV",
        "outputId": "1ad5f2f9-c24f-4347-c5f6-36b7941deaa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4307944 || all params: 90875600 || trainable%: 4.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "CbEL9xHXOtpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch._dynamo.list_backends()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLpPk1gmTAr5",
        "outputId": "d7ee6ad7-0542-4548-92cf-f7bfa4b8ec66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model_c = torch.compile(lora_model, backend = 'inductor')"
      ],
      "metadata": {
        "id": "MqsUMz_fOo46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load the evaluation metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy, precision, recall, and F1 score on a batch of predictions.\"\"\"\n",
        "    # Get the predictions and the true labels\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    references = eval_pred.label_ids\n",
        "\n",
        "    # Compute each metric\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=references)\n",
        "    precision = precision_metric.compute(predictions=predictions, references=references, average='weighted')\n",
        "    recall = recall_metric.compute(predictions=predictions, references=references, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=references, average='weighted')\n",
        "\n",
        "    # Return all metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"precision\": precision[\"precision\"],\n",
        "        \"recall\": recall[\"recall\"],\n",
        "        \"f1\": f1[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "ATMADCbQOr4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ],
      "metadata": {
        "id": "36DF9t45Oyfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import logging\n",
        "\n",
        "# Ensure the model is loaded on the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to GPU\n",
        "# lora_model = lora_model.to(device)\n",
        "\n",
        "lora_model_c = lora_model_c.to(device)\n",
        "\n",
        "# Enable logging and tqdm\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "from transformers.utils import logging as hf_logging\n",
        "hf_logging.set_verbosity_info()\n",
        "\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "batch_size = 128\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-lora-food101\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision only if GPU is available\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        "    label_names=[\"labels\"],\n",
        "    disable_tqdm=False,  # Disable tqdm progress bar\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    lora_model_c,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9VK-w18O0_C",
        "outputId": "546a0809-607e-4056-e3d6-e1b83511c085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "<ipython-input-20-3bbaa87a992f>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Using auto half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=f\"{model_name}-finetuning-food101-attentionheads_torchCompile_LoRA\",\n",
        "    group=\"LoRA-vs-Base\",\n",
        "    name=f\"{model_checkpoint}-run-{wandb.util.generate_id()}_Rank64\",\n",
        "    config=args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "4arksESDPCqe",
        "outputId": "05025d6e-81e6-4c4f-a778-8ceaab0fc850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:duiqaw6u) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">facebook/deit-base-patch16-224-run-ge905zpd_Rank64</strong> at: <a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/duiqaw6u' target=\"_blank\">https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/duiqaw6u</a><br/> View project at: <a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA' target=\"_blank\">https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241114_201511-duiqaw6u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:duiqaw6u). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241114_201906-i5vheowm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/i5vheowm' target=\"_blank\">facebook/deit-base-patch16-224-run-k7ld8no4_Rank64</a></strong> to <a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA' target=\"_blank\">https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/i5vheowm' target=\"_blank\">https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/i5vheowm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ab5640-columbia-university/deit-base-patch16-224-finetuning-food101-attentionheads_torchCompile_LoRA/runs/i5vheowm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ca091a20970>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "-durcoPaU1Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wRhXp_gZPYvt",
        "outputId": "7eda15ef-e755-41f1-d5b1-25a960a6f6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 4,500\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 45\n",
            "  Number of trainable parameters = 4,307,944\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45/45 05:46, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.273320</td>\n",
              "      <td>0.908000</td>\n",
              "      <td>0.909254</td>\n",
              "      <td>0.908000</td>\n",
              "      <td>0.907345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.426700</td>\n",
              "      <td>0.245476</td>\n",
              "      <td>0.898000</td>\n",
              "      <td>0.896974</td>\n",
              "      <td>0.898000</td>\n",
              "      <td>0.893909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.292300</td>\n",
              "      <td>0.260899</td>\n",
              "      <td>0.906000</td>\n",
              "      <td>0.906823</td>\n",
              "      <td>0.906000</td>\n",
              "      <td>0.904325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.238400</td>\n",
              "      <td>0.211795</td>\n",
              "      <td>0.918000</td>\n",
              "      <td>0.918548</td>\n",
              "      <td>0.918000</td>\n",
              "      <td>0.917711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.200500</td>\n",
              "      <td>0.199993</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.920229</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.919648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 128\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 831 \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:21.701000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 592 \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:31.639000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 443 \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:41.762000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 407 \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:42.458000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 337 \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.007000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py line 252 \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.467000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py line 559 \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2235, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 789, in fx_codegen_and_compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     _recursive_post_grad_passes(gm, is_inference=is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 288, in _recursive_post_grad_passes\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     post_grad_passes(gm, is_inference)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/post_grad.py\", line 100, in post_grad_passes\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     patterns.apply(gm.graph)  # type: ignore[arg-type]\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/pattern_matcher.py\", line 1729, in apply\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     if is_match(m) and entry.extra_check(m):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/fx_passes/quantization.py\", line 1448, in fn\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     scales = match.kwargs[\"scales\"].meta[\"val\"]\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] AttributeError: 'float' object has no attribute 'meta'\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1114 20:20:43.665000 10527 torch/_dynamo/convert_frame.py:1125] \n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-9\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-9/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-18\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-18/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-27\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-27/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-36\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-36/preprocessor_config.json\n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-45\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-45/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to deit-base-patch16-224-finetuned-lora-food101/checkpoint-45\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in deit-base-patch16-224-finetuned-lora-food101/checkpoint-45/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from deit-base-patch16-224-finetuned-lora-food101/checkpoint-45 (score: 0.92).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SZkIy3zVVl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}