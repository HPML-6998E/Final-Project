{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcvV4Y0jzHdz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from torch.ao.quantization import quantize_dynamic"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY0z1DZCWFsd",
        "outputId": "8eefccd2-b163-4999-98e8-c15b07caf596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.0\n",
            "    Uninstalling numpy-2.2.0:\n",
            "      Successfully uninstalled numpy-2.2.0\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "q4ASugrU0Q1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pretrained Model for Baseline Training\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    num_labels=101,\n",
        "    ignore_mismatched_sizes=True,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "uX1NlFd20TnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cb84d5-5958-497c-8b3f-8618444d3c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized Data Transform Pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "VMDXKfyM0bkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized Data Transform Pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Load Food101 Dataset\n",
        "train_dataset = datasets.Food101(root='./data', split='train', transform=transform, download=True)\n",
        "val_dataset = datasets.Food101(root='./data', split='test', transform=transform, download=True)\n",
        "test_dataset = datasets.Food101(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=torch.multiprocessing.cpu_count(),\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=torch.multiprocessing.cpu_count(),\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=torch.multiprocessing.cpu_count(),\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "eJmFjNpt1EG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "UVcQM6t10iQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop with Checkpoint Saving\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epochs=5, save_dir=\"model_checkpoints\"):\n",
        "    model.train()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
        "            for images, labels in tepoch:\n",
        "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images).logits\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                tepoch.set_postfix(loss=total_loss/total, accuracy=100 * correct/total)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
        "\n",
        "        # Save the model checkpoint after each epoch\n",
        "        model.save_pretrained(os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}\"))\n"
      ],
      "metadata": {
        "id": "l3wxubjI1Enn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Baseline Model for 5 (or more) Epochs\n",
        "train_model(model, train_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVL2joEl1FVQ",
        "outputId": "ce73b9d9-8432-4f41-d4a2-d3796bd0212a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2368/2368 [08:44<00:00,  4.52batch/s, accuracy=65, loss=0.06]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 1.9197, Accuracy: 64.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2368/2368 [08:42<00:00,  4.53batch/s, accuracy=85.4, loss=0.0201]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Loss: 0.6414, Accuracy: 85.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2368/2368 [08:42<00:00,  4.53batch/s, accuracy=91.2, loss=0.0119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Loss: 0.3805, Accuracy: 91.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2368/2368 [08:42<00:00,  4.53batch/s, accuracy=95.1, loss=0.00706]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Loss: 0.2258, Accuracy: 95.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2368/2368 [08:42<00:00,  4.53batch/s, accuracy=97.5, loss=0.004]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Loss: 0.1278, Accuracy: 97.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Size Function\n",
        "def save_model_size(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    size_in_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "    print(f\"Model Size: {size_in_mb:.2f} MB\")\n",
        "    return size_in_mb"
      ],
      "metadata": {
        "id": "k4KyJvTw3Pp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTForImageClassification.from_pretrained(\"model_checkpoints/checkpoint_epoch_5\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "zhAq1qtk8W7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7fe585-fae8-49dc-f047-41041a6d278b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    correct, total = 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            outputs = model(images).logits\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "JHv8Y2a73Rzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the accuray and other results above in a list\n",
        "baseline_results = evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWvaxWPd7xbF",
        "outputId": "586ca037-51f2-451d-d682-21dee66442ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 790/790 [00:57<00:00, 13.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8764\n",
            "Precision: 0.8779\n",
            "Recall:    0.8764\n",
            "F1 Score:  0.8764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(baseline_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDoav5ph7-7J",
        "outputId": "f54f0163-d897-4a5e-81d1-dd455ec96939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.8763960396039604, 0.8778630341015732, 0.8763960396039604, 0.8764282967683963)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training Quantization"
      ],
      "metadata": {
        "id": "vvg9Dr9Q6h3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall bitsandbytes -y\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "t78D4nTo8oUr",
        "outputId": "faf06547-9f5c-4a57-95e5-bd7ec46bb77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.45.0\n",
            "Uninstalling bitsandbytes-0.45.0:\n",
            "  Successfully uninstalled bitsandbytes-0.45.0\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "bitsandbytes"
                ]
              },
              "id": "ccfd86de256f47c0948de54f3d4c6046"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig, ViTForImageClassification\n",
        "import bitsandbytes as bnb"
      ],
      "metadata": {
        "id": "HM2ZKovX9BSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "print(bnb.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdtnHyV29Wyu",
        "outputId": "f83bf8c9-b911-43c4-ac8b-37f93230f0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization\n",
        "    llm_int8_threshold=6.0,  # Ensure proper layer quantization\n",
        "    llm_int8_enable_fp32_cpu_offload=True,  # Offload FP32 layers to CPU\n",
        "    llm_int8_skip_modules=[\"LayerNorm\", \"Embeddings\"]  # Keep critical layers in FP32\n",
        ")"
      ],
      "metadata": {
        "id": "O39b-Dcv89nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fine-Tuned Model and Apply PTQ\n",
        "ptq_model = ViTForImageClassification.from_pretrained(\n",
        "    \"model_checkpoints/checkpoint_epoch_5\",\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    local_files_only=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "sH5hH4_53UNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IuGYZOxaAJ1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantized Layers (INT8) PTQ\n",
        "\n",
        "    Fully Connected Layers:\n",
        "        torch.nn.Linear layers within:\n",
        "            Vision Transformer Encoder Layers\n",
        "            Feed-Forward Network (FFN) Layers\n",
        "        These layers are the most memory-intensive, making them ideal for INT8 quantization.\n",
        "\n",
        "    Attention Mechanism Layers:\n",
        "        Self-Attention Projections: Query, Key, and Value projections are quantized.\n",
        "        Attention Output Layers: Quantization of attention heads reduces memory use significantly."
      ],
      "metadata": {
        "id": "uzTtjKZsLTJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Size\n",
        "original_model_size = save_model_size(model, \"original_model_size\")\n",
        "mixed_precision_size = save_model_size(ptq_model, \"mixed_precision_model_size\")\n",
        "\n",
        "# Evaluate the Quantized Model\n",
        "original_metrics = evaluate_model(model, test_loader, device)\n",
        "mixed_precision_metrics = evaluate_model(ptq_model, test_loader, device)"
      ],
      "metadata": {
        "id": "1XYeZEVr3U4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2233f0-c969-4ec2-c03f-1c72fafed1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 327.67 MB\n",
            "Model Size: 83.17 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 790/790 [00:57<00:00, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8764\n",
            "Precision: 0.8779\n",
            "Recall:    0.8764\n",
            "F1 Score:  0.8764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 790/790 [00:42<00:00, 18.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7997\n",
            "Precision: 0.8174\n",
            "Recall:    0.7997\n",
            "F1 Score:  0.7994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Comparison\n",
        "print(\"\\n=== Model Summary ===\")\n",
        "print(f\"Post-Training Quantized Metrics: Accuracy={mixed_precision_metrics[0]:.4f}, Precision={mixed_precision_metrics[1]:.4f}, Recall={mixed_precision_metrics[2]:.4f}, F1-Score={mixed_precision_metrics[3]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah04W65K-74L",
        "outputId": "0e295c66-7398-4344-d471-5eceeeaf1e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Summary ===\n",
            "Post-Training Quantized Metrics: Accuracy=0.7997, Precision=0.8174, Recall=0.7997, F1-Score=0.7994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zK-GwwOW3ZQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PV-i7BRY3buX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}